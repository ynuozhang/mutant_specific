{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import pickle\n",
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096' # do this before importing pytorch\n",
    "import torch\n",
    "import esm\n",
    "from torch.utils.data import TensorDataset\n",
    "from esm import Alphabet, FastaBatchedDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:4096\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Retrieve the value of the environment variable\n",
    "value = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', None)\n",
    "# Print the value\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF:\", value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/high_confidence_ppi.csv', index_col=0)\n",
    "columns_to_keep = ['UniProtID_1', 'UniProtID_2', 'symbol_1', 'symbol_2', 'seq_1', 'seq_2',\n",
    "                   'Experimental System', 'Throughput', 'len_1', 'len_2', 'Pubmed ID']\n",
    "df = df[columns_to_keep]\n",
    "df.drop_duplicates(inplace=True)  # no duplicate\n",
    "# drop rows where either seq1 or seq2 has string length > 2000\n",
    "df = df[~((df['seq_1'].str.len() > 2000) | (df['seq_2'].str.len() > 2000))]\n",
    "# generate embeddings\n",
    "seqs_wt1 = df.seq_1.values.tolist()\n",
    "seqs_wt2 = df.seq_2.values.tolist()\n",
    "seqs_wt1 = set(seqs_wt1)\n",
    "seqs_wt2 = set(seqs_wt2)\n",
    "seqs_labeled_wt1 = []\n",
    "count = 0\n",
    "for seq in seqs_wt1:\n",
    "    seqs_labeled_wt1.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "seqs_labeled_wt2 = []\n",
    "count = 0\n",
    "for seq in seqs_wt2:\n",
    "    seqs_labeled_wt2.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "# alternative way to generate batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "dataset = FastaBatchedDataset(list(zip(*seqs_labeled_wt1))[0], list(zip(*seqs_labeled_wt1))[1])\n",
    "batches = dataset.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          collate_fn=Alphabet.from_architecture(\"roberta_large\").get_batch_converter(),\n",
    "                                          batch_sampler=batches, pin_memory=True)\n",
    "dataset_seq2 = FastaBatchedDataset(list(zip(*seqs_labeled_wt2))[0], list(zip(*seqs_labeled_wt2))[1])\n",
    "batches_seq2 = dataset_seq2.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader_seq2 = torch.utils.data.DataLoader(dataset_seq2, collate_fn=Alphabet.from_architecture(\n",
    "    \"roberta_large\").get_batch_converter(), batch_sampler=batches_seq2, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1989"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(list(zip(*seqs_labeled_wt1))[1], key=len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gonna use 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": "1989"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ESM-2 model\n",
    "#del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('gonna use', torch.cuda.device_count(), 'GPUs!')\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "# find longest sequence\n",
    "len(max(list(zip(*seqs_labeled_wt1))[1], key=len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = torch.nn.DataParallel(model) # use all GPUs by default\n",
    "# splitting the input batch into GPUs\n",
    "model.cuda()\n",
    "print('Transferred model to GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04 11:30:06.318697 | Batch ID: 0['seq1179', 'seq253', 'seq1744', 'seq2455', 'seq2789', 'seq582', 'seq3220', 'seq130', 'seq1974', 'seq1821', 'seq194', 'seq323', 'seq481', 'seq2210']14\n",
      "2023-09-04 11:30:06.321901 | finish generating embeddings\n",
      "2023-09-04 11:30:13.764490 | Batch ID: 1['seq1617', 'seq1145', 'seq2509', 'seq1806', 'seq652', 'seq1223', 'seq2922', 'seq852', 'seq2101', 'seq3062', 'seq3063', 'seq2054']12\n",
      "2023-09-04 11:30:13.767056 | finish generating embeddings\n",
      "2023-09-04 11:30:21.040868 | Batch ID: 2['seq482', 'seq2270', 'seq3371', 'seq1479', 'seq2104', 'seq2582', 'seq326', 'seq1487', 'seq1608', 'seq2747', 'seq3532']11\n",
      "2023-09-04 11:30:21.043586 | finish generating embeddings\n",
      "2023-09-04 11:30:33.315545 | Batch ID: 3['seq1789', 'seq1232', 'seq1300', 'seq2325', 'seq235', 'seq2173', 'seq2702', 'seq3319', 'seq254', 'seq2657']10\n",
      "2023-09-04 11:30:33.317019 | finish generating embeddings\n",
      "2023-09-04 11:30:40.517603 | Batch ID: 4['seq2016', 'seq3038', 'seq3108', 'seq807', 'seq1631', 'seq2895', 'seq3009', 'seq2490', 'seq2840', 'seq2019']10\n",
      "2023-09-04 11:30:40.520144 | finish generating embeddings\n",
      "2023-09-04 11:30:47.908398 | Batch ID: 5['seq2206', 'seq2211', 'seq2695', 'seq2066', 'seq2379', 'seq2890', 'seq233', 'seq1613', 'seq2573', 'seq3087']10\n",
      "2023-09-04 11:30:47.910794 | finish generating embeddings\n",
      "2023-09-04 11:30:57.315387 | Batch ID: 6['seq98', 'seq1298', 'seq1565', 'seq1832', 'seq2531', 'seq1091', 'seq2159', 'seq1056', 'seq1471']9\n",
      "2023-09-04 11:30:57.317851 | finish generating embeddings\n",
      "2023-09-04 11:31:04.458780 | Batch ID: 7['seq1862', 'seq2380', 'seq2574', 'seq2249', 'seq649', 'seq1440', 'seq2697', 'seq741', 'seq690']9\n",
      "2023-09-04 11:31:04.461109 | finish generating embeddings\n",
      "2023-09-04 11:31:12.087796 | Batch ID: 8['seq1040', 'seq2051', 'seq2077', 'seq2493', 'seq3020', 'seq671', 'seq1774', 'seq1793']8\n",
      "2023-09-04 11:31:12.090538 | finish generating embeddings\n",
      "2023-09-04 11:31:19.329109 | Batch ID: 9['seq2623', 'seq1074', 'seq174', 'seq604', 'seq817', 'seq1803', 'seq2035', 'seq3531']8\n",
      "2023-09-04 11:31:19.331534 | finish generating embeddings\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (labels, strs, toks) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#if torch.cuda.is_available():\u001B[39;00m\n\u001B[1;32m      4\u001B[0m      \u001B[38;5;66;03m#   toks = toks.to(device)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m----> 6\u001B[0m         results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepr_layers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m33\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_contacts\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepresentations\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m33\u001B[39m]\n\u001B[1;32m      7\u001B[0m     results_cpu \u001B[38;5;241m=\u001B[39m results\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m#torch.cuda.empty_cache()\u001B[39;00m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001B[0m, in \u001B[0;36mDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39minputs[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    167\u001B[0m replicas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplicate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids[:\u001B[38;5;28mlen\u001B[39m(inputs)])\n\u001B[0;32m--> 168\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallel_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather(outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_device)\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001B[0m, in \u001B[0;36mDataParallel.parallel_apply\u001B[0;34m(self, replicas, inputs, kwargs)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparallel_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, replicas, inputs, kwargs):\n\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparallel_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:78\u001B[0m, in \u001B[0;36mparallel_apply\u001B[0;34m(modules, inputs, kwargs_tup, devices)\u001B[0m\n\u001B[1;32m     76\u001B[0m         thread\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m thread \u001B[38;5;129;01min\u001B[39;00m threads:\n\u001B[0;32m---> 78\u001B[0m         \u001B[43mthread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m     _worker(\u001B[38;5;241m0\u001B[39m, modules[\u001B[38;5;241m0\u001B[39m], inputs[\u001B[38;5;241m0\u001B[39m], kwargs_tup[\u001B[38;5;241m0\u001B[39m], devices[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py:1011\u001B[0m, in \u001B[0;36mThread.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1008\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1011\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1013\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[1;32m   1014\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py:1027\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# already determined that the C code is done\u001B[39;00m\n\u001B[1;32m   1026\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_stopped\n\u001B[0;32m-> 1027\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1028\u001B[0m     lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "representation_store_dict = {}\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    #if torch.cuda.is_available():\n",
    "     #   toks = toks.to(device)\n",
    "    with torch.no_grad():\n",
    "        results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "    results_cpu = results.to(device='cpu')\n",
    "    #torch.cuda.empty_cache()\n",
    "    tprint('Batch ID: '+str(batch_idx)+str(labels)+str(len(strs)))\n",
    "    #tprint(torch.cuda.memory_allocated())\n",
    "    #tprint(torch.cuda.memory_snapshot())\n",
    "    for i, str_ in enumerate(strs):\n",
    "        # only select representations relate to the sequence\n",
    "        # rest of the sequences are paddings, check notebook\n",
    "        # create dictionary {sequence: embeddings}\n",
    "        representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()\n",
    "    tprint('finish generating embeddings')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import datetime, sys\n",
    "def tprint(string):\n",
    "    string = str(string)\n",
    "    sys.stdout.write(str(datetime.datetime.now()) + ' | ')\n",
    "    sys.stdout.write(string + '\\n')\n",
    "    sys.stdout.flush()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}