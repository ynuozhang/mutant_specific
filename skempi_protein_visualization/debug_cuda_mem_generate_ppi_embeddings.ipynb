{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import pickle\n",
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096' # do this before importing pytorch\n",
    "import torch\n",
    "import esm\n",
    "from torch.utils.data import TensorDataset\n",
    "from esm import Alphabet, FastaBatchedDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:4096\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Retrieve the value of the environment variable\n",
    "value = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', None)\n",
    "# Print the value\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF:\", value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/high_confidence_ppi.csv', index_col=0)\n",
    "columns_to_keep = ['UniProtID_1', 'UniProtID_2', 'symbol_1', 'symbol_2', 'seq_1', 'seq_2',\n",
    "                   'Experimental System', 'Throughput', 'len_1', 'len_2', 'Pubmed ID']\n",
    "df = df[columns_to_keep]\n",
    "df.drop_duplicates(inplace=True)  # no duplicate\n",
    "# drop rows where either seq1 or seq2 has string length > 2000\n",
    "df = df[~((df['seq_1'].str.len() > 2000) | (df['seq_2'].str.len() > 2000))]\n",
    "# generate embeddings\n",
    "seqs_wt1 = df.seq_1.values.tolist()\n",
    "seqs_wt2 = df.seq_2.values.tolist()\n",
    "seqs_wt1 = set(seqs_wt1)\n",
    "seqs_wt2 = set(seqs_wt2)\n",
    "seqs_labeled_wt1 = []\n",
    "count = 0\n",
    "for seq in seqs_wt1:\n",
    "    seqs_labeled_wt1.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "seqs_labeled_wt2 = []\n",
    "count = 0\n",
    "for seq in seqs_wt2:\n",
    "    seqs_labeled_wt2.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "# alternative way to generate batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "dataset = FastaBatchedDataset(list(zip(*seqs_labeled_wt1))[0], list(zip(*seqs_labeled_wt1))[1])\n",
    "batches = dataset.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          collate_fn=Alphabet.from_architecture(\"roberta_large\").get_batch_converter(),\n",
    "                                          batch_sampler=batches, pin_memory=True)\n",
    "dataset_seq2 = FastaBatchedDataset(list(zip(*seqs_labeled_wt2))[0], list(zip(*seqs_labeled_wt2))[1])\n",
    "batches_seq2 = dataset_seq2.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader_seq2 = torch.utils.data.DataLoader(dataset_seq2, collate_fn=Alphabet.from_architecture(\n",
    "    \"roberta_large\").get_batch_converter(), batch_sampler=batches_seq2, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1989"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(list(zip(*seqs_labeled_wt1))[1], key=len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import datetime, sys\n",
    "def tprint(string):\n",
    "    string = str(string)\n",
    "    sys.stdout.write(str(datetime.datetime.now()) + ' | ')\n",
    "    sys.stdout.write(string + '\\n')\n",
    "    sys.stdout.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ESM2(\n  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n  (layers): ModuleList(\n    (0): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (6): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (7): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (8): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (9): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (10): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (11): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (12): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (13): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (14): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (15): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (16): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (17): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (18): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (19): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (20): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (21): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (22): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (23): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (24): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (25): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (26): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (27): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (28): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (29): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (30): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (31): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (32): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (contact_head): ContactPredictionHead(\n    (regression): Linear(in_features=660, out_features=1, bias=True)\n    (activation): Sigmoid()\n  )\n  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try batch inspection instead\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "# find longest sequence\n",
    "torch.cuda.empty_cache()\n",
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "2200"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2115 ['seq2482'] ['MEPRMESCLAQVLQKDVGKRLQVGQELIDYFSDKQKSADLEHDQTMLDKLVDGLATSWVNSSNYKVVLLGMDILSALVTRLQDRFKAQIGTVLPSLIDRLGDAKDSVREQDQTLLLKIMDQAANPQYVWDRMLGGFKHKNFRTREGICLCLIATLNASGAQTLTLSKIVPHICNLLGDPNSQVRDAAINSLVEIYRHVGERVRADLSKKGLPQSRLNVIFTKFDEVQKSGNMIQSANDKNFDDEDSVDGNRPSSASSTSSKAPPSSRRNVGMGTTRRLGSSTLGSKSSAAKEGAGAVDEEDFIKAFDDVPVVQIYSSRDLEESINKIREILSDDKHDWEQRVNALKKIRSLLLAGAAEYDNFFQHLRLLDGAFKLSAKDLRSQVVREACITLGHLSSVLGNKFDHGAEAIMPTIFNLIPNSAKIMATSGVVAVRLIIRHTHIPRLIPVITSNCTSKSVAVRRRCFEFLDLLLQEWQTHSLERHISVLAETIKKGIHDADSEARIEARKCYWGFHSHFSREAEHLYHTLESSYQKALQSHLKNSDSIVSLPQSDRSSSSSQESLNRPLSAKRSPTGSTTSRASTVSTKSVSTTGSLQRSRSDIDVNAAASAKSKVSSSSGTTPFSSAAALPPGSYASLGRIRTRRQSSGSATNVASTPDNRGRSRAKVVSQSQRSRSANPAGAGSRSSSPGKLLGSGYGGLTGGSSRGPPVTPSSEKRSKIPRSQGCSRETSPNRIGLARSSRIPRPSMSQGCSRDTSRESSRDTSPARGFPPLDRFGLGQPGRIPGSVNAMRVLSTSTDLEAAVADALKKPVRRRYEPYGMYSDDDANSDASSVCSERSYGSRNGGIPHYLRQTEDVAEVLNHCASSNWSERKEGLLGLQNLLKSQRTLSRVELKRLCEIFTRMFADPHSKRVFSMFLETLVDFIIIHKDDLQDWLFVLLTQLLKKMGADLLGSVQAKVQKALDVTRDSFPFDQQFNILMRFIVDQTQTPNLKVKVAILKYIESLARQMDPTDFVNSSETRLAVSRIITWTTEPKSSDVRKAAQIVLISLFELNTPEFTMLLGALPKTFQDGATKLLHNHLKNSSNTSVGSPSNTIGRTPSRHTSSRTSPLTSPTNCSHGGLSPSRLWGWSADGLAKHPPPFSQPNSIPTAPSHKALRRSYSPSMLDYDTENLNSEEIYSSLRGVTEAIEKFSFRSQEDLNEPIKRDGKKECDIVSRDGGAASPATEGRGGSEVEGGRTALDNKTSLLNTQPPRAFPGPRARDYNPYPYSDAINTYDKTALKEAVFDDDMEQLRDVPIDHSDLVADLLKELSNHNERVEERKGALLELLKITREDSLGVWEEHFKTILLLLLETLGDKDHSIRALALRVLREILRNQPARFKNYAELTIMKTLEAHKDSHKEVVRAAEEAASTLASSIHPEQCIKVLCPIIQTADYPINLAAIKMQTKVVERIAKESLLQLLVDIIPGLLQGYDNTESSVRKASVFCLVAIYSVIGEDLKPHLAQLTGSKMKLLNLYIKRAQTTNSNSSSSSDVSTHS'] 1538\n"
     ]
    }
   ],
   "source": [
    "# QC\n",
    "start_batch_id = 1188\n",
    "end_batch_id = 1615\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    #if start_batch_id <= batch_idx <= end_batch_id:\n",
    "    if len(strs[0]) == 1538:\n",
    "        print(batch_idx, labels, strs, len(strs[0]))\n",
    "        #break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Get the current GPU memory usage.\n",
    "\n",
    "    Returns:\n",
    "        allocated (float): Memory allocated by tensors in GB.\n",
    "        cached (float): Cached memory in GB.\n",
    "    \"\"\"\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside cache\n",
    "    return a/1024**3, r/1024**3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04 16:10:08.574704 | Batch 2115: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.82 GiB (GPU 0; 39.59 GiB total capacity; 31.67 GiB already allocated; 5.78 GiB free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m start_allocated, start_cached \u001B[38;5;241m=\u001B[39m get_gpu_memory()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 13\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepr_layers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m33\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_contacts\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepresentations\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m33\u001B[39m]\n\u001B[1;32m     14\u001B[0m end_allocated, end_cached \u001B[38;5;241m=\u001B[39m get_gpu_memory()\n\u001B[1;32m     15\u001B[0m tprint(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Allocated memory increased by \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(end_allocated \u001B[38;5;241m-\u001B[39m start_allocated)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m GB, Cached memory increased by \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(end_cached \u001B[38;5;241m-\u001B[39m start_cached)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m GB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/esm/model/esm2.py:141\u001B[0m, in \u001B[0;36mESM2.forward\u001B[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001B[0m\n\u001B[1;32m    139\u001B[0m     result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattentions\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m attentions\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_contacts:\n\u001B[0;32m--> 141\u001B[0m         contacts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontact_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m         result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontacts\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m contacts\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/esm/modules.py:355\u001B[0m, in \u001B[0;36mContactPredictionHead.forward\u001B[0;34m(self, tokens, attentions)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# features: B x C x T x T\u001B[39;00m\n\u001B[1;32m    352\u001B[0m attentions \u001B[38;5;241m=\u001B[39m attentions\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregression\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    354\u001B[0m )  \u001B[38;5;66;03m# attentions always float32, may need to convert to float16\u001B[39;00m\n\u001B[0;32m--> 355\u001B[0m attentions \u001B[38;5;241m=\u001B[39m \u001B[43mapc\u001B[49m\u001B[43m(\u001B[49m\u001B[43msymmetrize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattentions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m attentions \u001B[38;5;241m=\u001B[39m attentions\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregression(attentions)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m3\u001B[39m))\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/esm/modules.py:40\u001B[0m, in \u001B[0;36mapc\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     38\u001B[0m avg \u001B[38;5;241m=\u001B[39m a1 \u001B[38;5;241m*\u001B[39m a2\n\u001B[1;32m     39\u001B[0m avg\u001B[38;5;241m.\u001B[39mdiv_(a12)  \u001B[38;5;66;03m# in-place to reduce memory\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m normalized \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mavg\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m normalized\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 5.82 GiB (GPU 0; 39.59 GiB total capacity; 31.67 GiB already allocated; 5.78 GiB free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "representation_store_dict = {}\n",
    "start_batch_id = 2115\n",
    "end_batch_id = 2200\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    start_allocated, start_cached = get_gpu_memory()\n",
    "    if start_batch_id <= batch_idx <= end_batch_id:\n",
    "        if torch.cuda.is_available():\n",
    "            toks = toks.to(device='cuda', non_blocking=True)\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        with torch.no_grad():\n",
    "            results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        results_cpu = results.to(device='cpu')\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        del results, toks\n",
    "        torch.cuda.empty_cache()\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        tprint('Batch ID: '+str(batch_idx)+str(labels)+str(len(strs)))\n",
    "        #tprint(torch.cuda.memory_allocated())\n",
    "        #tprint(torch.cuda.memory_snapshot())\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        for i, str_ in enumerate(strs):\n",
    "        # only select representations relate to the sequence\n",
    "        # rest of the sequences are paddings, check notebook\n",
    "        # create dictionary {sequence: embeddings}\n",
    "            representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        tprint('finish generating embeddings')\n",
    "    elif batch_idx > end_batch_id:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ESM2(\n  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n  (layers): ModuleList(\n    (0): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (6): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (7): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (8): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (9): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (10): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (11): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (12): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (13): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (14): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (15): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (16): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (17): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (18): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (19): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (20): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (21): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (22): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (23): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (24): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (25): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (26): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (27): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (28): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (29): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (30): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (31): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (32): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (rot_emb): RotaryEmbedding()\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (contact_head): ContactPredictionHead(\n    (regression): Linear(in_features=660, out_features=1, bias=True)\n    (activation): Sigmoid()\n  )\n  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" debug, really hardware issue? running stuffs on CPU where there's plenty memory \"\"\"\n",
    "# try batch inspection instead\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "# find longest sequence\n",
    "len(max(list(zip(*seqs_labeled_wt1))[1], key=len))\n",
    "model.to('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04 16:47:08.397970 | Batch ID: 2115['seq816']['MEPRMESCLAQVLQKDVGKRLQVGQELIDYFSDKQKSADLEHDQTMLDKLVDGLATSWVNSSNYKVVLLGMDILSALVTRLQDRFKAQIGTVLPSLIDRLGDAKDSVREQDQTLLLKIMDQAANPQYVWDRMLGGFKHKNFRTREGICLCLIATLNASGAQTLTLSKIVPHICNLLGDPNSQVRDAAINSLVEIYRHVGERVRADLSKKGLPQSRLNVIFTKFDEVQKSGNMIQSANDKNFDDEDSVDGNRPSSASSTSSKAPPSSRRNVGMGTTRRLGSSTLGSKSSAAKEGAGAVDEEDFIKAFDDVPVVQIYSSRDLEESINKIREILSDDKHDWEQRVNALKKIRSLLLAGAAEYDNFFQHLRLLDGAFKLSAKDLRSQVVREACITLGHLSSVLGNKFDHGAEAIMPTIFNLIPNSAKIMATSGVVAVRLIIRHTHIPRLIPVITSNCTSKSVAVRRRCFEFLDLLLQEWQTHSLERHISVLAETIKKGIHDADSEARIEARKCYWGFHSHFSREAEHLYHTLESSYQKALQSHLKNSDSIVSLPQSDRSSSSSQESLNRPLSAKRSPTGSTTSRASTVSTKSVSTTGSLQRSRSDIDVNAAASAKSKVSSSSGTTPFSSAAALPPGSYASLGRIRTRRQSSGSATNVASTPDNRGRSRAKVVSQSQRSRSANPAGAGSRSSSPGKLLGSGYGGLTGGSSRGPPVTPSSEKRSKIPRSQGCSRETSPNRIGLARSSRIPRPSMSQGCSRDTSRESSRDTSPARGFPPLDRFGLGQPGRIPGSVNAMRVLSTSTDLEAAVADALKKPVRRRYEPYGMYSDDDANSDASSVCSERSYGSRNGGIPHYLRQTEDVAEVLNHCASSNWSERKEGLLGLQNLLKSQRTLSRVELKRLCEIFTRMFADPHSKRVFSMFLETLVDFIIIHKDDLQDWLFVLLTQLLKKMGADLLGSVQAKVQKALDVTRDSFPFDQQFNILMRFIVDQTQTPNLKVKVAILKYIESLARQMDPTDFVNSSETRLAVSRIITWTTEPKSSDVRKAAQIVLISLFELNTPEFTMLLGALPKTFQDGATKLLHNHLKNSSNTSVGSPSNTIGRTPSRHTSSRTSPLTSPTNCSHGGLSPSRLWGWSADGLAKHPPPFSQPNSIPTAPSHKALRRSYSPSMLDYDTENLNSEEIYSSLRGVTEAIEKFSFRSQEDLNEPIKRDGKKECDIVSRDGGAASPATEGRGGSEVEGGRTALDNKTSLLNTQPPRAFPGPRARDYNPYPYSDAINTYDKTALKEAVFDDDMEQLRDVPIDHSDLVADLLKELSNHNERVEERKGALLELLKITREDSLGVWEEHFKTILLLLLETLGDKDHSIRALALRVLREILRNQPARFKNYAELTIMKTLEAHKDSHKEVVRAAEEAASTLASSIHPEQCIKVLCPIIQTADYPINLAAIKMQTKVVERIAKESLLQLLVDIIPGLLQGYDNTESSVRKASVFCLVAIYSVIGEDLKPHLAQLTGSKMKLLNLYIKRAQTTNSNSSSSSDVSTHS']\n",
      "2023-09-04 16:47:49.299354 | Batch ID: 2116['seq1219']['MDSYFKAAVSDLDKLLDDFEQNPDEQDYLQDVQNAYDSNHCSVSSELASSQRTSLLPKDQECVNSCASSETSYGTNESSLNEKTLKGLTSIQNEKNVTGLDLLSSVDGGTSDEIQPLYMGRCSKPICDLISDMGNLVHATNSEEDIKKLLPDDFKSNADSLIGLDLSSVSDTPCVSSTDHDSDTVREQQNDISSELQNREIGGIKELGIKVDTTLSDSYNYSGTENLKDKKIFNQLESIVDFNMSSALTRQSSKMFHAKDKLQHKSQPCGLLKDVGLVKEEVDVAVITAAECLKEEGKTSALTCSLPKNEDLCLNDSNSRDENFKLPDFSFQEDKTVIKQSAQEDSKSLDLKDNDVIQDSSSALHVSSKDVPSSLSCLPASGSMCGSLIESKARGDFLPQHEHKDNIQDAVTIHEEIQNSVVLGGEPFKENDLLKQEKCKSILLQSLIEGMEDRKIDPDQTVIRAESLDGGDTSSTVVESQEGLSGTHVPESSDCCEGFINTFSSNDMDGQDLDYFNIDEGAKSGPLISDAELDAFLTEQYLQTTNIKSFEENVNDSKSQMNQIDMKGLDDGNINNIYFNAEAGAIGESHGINIICEIVDKQNTIENGLSLGEKSTIPVQQGLPTSKSEITNQLSVSDINSQSVGGARPKQLFSLPSRTRSSKDLNKPDVPDTIESEPSTADTVVPITCAIDSTADPQVSFNSNYIDIESNSEGGSSFVTANEDSVPENTCKEGLVLGQKQPTWVPDSEAPNCMNCQVKFTFTKRRHHCRACGKVFCGVCCNRKCKLQYLEKEARVCVVCYETISKAQAFERMMSPTGSNLKSNHSDECTTVQPPQENQTSSIPSPATLPVSALKQPGVEGLCSKEQKRVWFADGILPNGEVADTTKLSSGSKRCSEDFSPLSPDVPMTVNTVDHSHSTTVEKPNNETGDITRNEIIQSPISQVPSVEKLSMNTGNEGLPTSGSFTLDDDVFAETEEPSSPTGVLVNSNLPIASISDYRLLCDINKYVCNKISLLPNDEDSLPPLLVASGEKGSVPVVEEHPSHEQIILLLEGESFHPVTFVLNANLLVNVKFIFYSSDKYWYFSTNGLHGLGQAEIIILLLCLPNEDTIPKDIFRLFITIYKDALKGKYIENLDNITFTESFLSSKDHGGFLFITPTFQKLDDLSLPSNPFLCGILIQKLEIPWAKVFPMRLMLRLGAEYKAYPAPLTSIRGRKPLFGEIGHTIMNLLVDLRNYQYTLHNIDQLLIHMEMGKSCIKIPRKKYSDVMKVLNSSNEHVISIGASFSTEADSHLVCIQNDGIYETQANSATGHPRKVTGASFVVFNGALKTSSGFLAKSSIVEDGLMVQITPETMNGLRLALREQKDFKITCGKVDAVDLREYVDICWVDAEEKGNKGVISSVDGISLQGFPSEKIKLEADFETDEKIVKCTEVFYFLKDQDLSILSTSYQFAKEIAMACSAALCPHLKTLKSNGMNKIGLRVSIDTDMVEFQAGSEGQLLPQHYLNDLDSALIPVIHGGTSNSSLPLEIELVFFIIEHLF']\n"
     ]
    }
   ],
   "source": [
    "representation_store_dict = {}\n",
    "start_batch_id = 2115\n",
    "end_batch_id = 2200\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    if start_batch_id <= batch_idx <= end_batch_id:\n",
    "        #if torch.cuda.is_available():\n",
    "            #toks = toks.to(device='cuda', non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            results_cpu = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "        #results_cpu = results.to(device='cpu')\n",
    "        #del results_cpu\n",
    "        tprint('Batch ID: '+str(batch_idx)+str(labels)+str(strs))\n",
    "        for i, str_ in enumerate(strs):\n",
    "            # only select representations relate to the sequence\n",
    "            # rest of the sequences are paddings, check notebook\n",
    "            # create dictionary {sequence: embeddings}\n",
    "            representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()\n",
    "    elif batch_idx > end_batch_id:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "      UniProtID_1 UniProtID_2     symbol_1     symbol_2  \\\n0          P67809      Q13242  YBOX1_HUMAN  SRSF9_HUMAN   \n1          Q8NER1      Q86SS6  TRPV1_HUMAN   SYT9_HUMAN   \n2          O75319      Q13242  DUS11_HUMAN  SRSF9_HUMAN   \n3          Q13526      Q7Z5J4   PIN1_HUMAN   RAI1_HUMAN   \n4          Q13526      Q9UJY4   PIN1_HUMAN   GGA2_HUMAN   \n...           ...         ...          ...          ...   \n24516      P60763      Q13443   RAC3_HUMAN  ADAM9_HUMAN   \n24517      P60763      Q9UNK0   RAC3_HUMAN   STX8_HUMAN   \n24518      Q9BQ90      Q15843  KLDC3_HUMAN  NEDD8_HUMAN   \n24519      Q9BWK5      Q9H9Q4  CYREN_HUMAN  NHEJ1_HUMAN   \n24520      Q9BR09      P13533  NEUL2_HUMAN   MYH6_HUMAN   \n\n                                                   seq_1  \\\n0      MSSEAETQQPPAAPPAAPALSAADTKPGTTGSGAGSGGPGGLTSAA...   \n1      MKKWSSTDLGAAADPLQKDTCPDPLDGDPNSRPPPAKPQLSTAKSR...   \n2      MRNSETLERGVGGCRVFSCLGSYPGIEGAGLALLADLALGGRLLGT...   \n3      MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNSSSGGK...   \n4      MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNSSSGGK...   \n...                                                  ...   \n24516  MQAIKCVVVGDGAVGKTCLLISYTTNAFPGEYIPTVFDNYSANVMV...   \n24517  MQAIKCVVVGDGAVGKTCLLISYTTNAFPGEYIPTVFDNYSANVMV...   \n24518  MLRWTVHLEGGPRRVNHAAVAVGHRVYSFGGYCSGEDYETLRQIDV...   \n24519  METLQSETKTRVLPSWLTAQVATKNVAPMKAPKRMRMAAVPVAAAR...   \n24520  MAAASEPVDSGALWGLERPEPPPTRFHRVHGANIRVDPSGTRATRV...   \n\n                                                   seq_2  \\\n0      MSGWADERGGEGDGRIYVGNLPTDVREKDLEDLFYKYGRIREIELK...   \n1      MPGARDALCHQALQLLAELCARGALEHDSCQDFIYHLRDRARPRLR...   \n2      MSGWADERGGEGDGRIYVGNLPTDVREKDLEDLFYKYGRIREIELK...   \n3      MQSFRERCGFHGKQQNYQQTSQETSRLENYRQPSQAGLSCDRQRLL...   \n4      MAATAVAAAVAGTESAQGPPGPAASLELWLNKATDPSMSEQDWSAI...   \n...                                                  ...   \n24516  MGSGARFPSGTLRVRWLLLLGLVGPVLGAARPGFQQTSHLSSYEII...   \n24517  MAPDPWFSTYDSTCQIAQEIAEKIQQRNQYERKGEKAPKLTVTIRA...   \n24518  MLIKVKTLTGKEIEIDIEPTDKVERIKERVEEKEGIPPQQQRLIYS...   \n24519  MEELEQGLLMQPWAWLQLAENSLLAKVFITKQGYALLVSDLQQVWH...   \n24520  MTDAQMADFGAAAQYLRKSEKERLEAQTRPFDIRTECFVPDDKEEF...   \n\n            Experimental System       Throughput  len_1  len_2   Pubmed ID  \\\n0                    Two-hybrid   Low Throughput    324    221  12604611.0   \n1                    Two-hybrid   Low Throughput    839    491  15066994.0   \n2                    Two-hybrid   Low Throughput    377    221   9685386.0   \n3                    Two-hybrid  High Throughput    163   1906  16189514.0   \n4                    Two-hybrid  High Throughput    163    613  16189514.0   \n...                         ...              ...    ...    ...         ...   \n24516       Affinity Capture-MS   Low Throughput    192    819  31871319.0   \n24517       Affinity Capture-MS   Low Throughput    192    236  31871319.0   \n24518       Affinity Capture-MS   Low Throughput    382     81  35468939.0   \n24519  Affinity Capture-Western   Low Throughput    157    299  30017584.0   \n24520      Biochemical Activity   Low Throughput    285   1939  35551201.0   \n\n                                   wild_seq_1_embeddings  \n0      [[0.020276865, -0.055094432, -0.05752476, 0.06...  \n1      [[0.005592045, -0.07453048, -0.054872476, 0.06...  \n2      [[-0.006202125, -0.0743498, 0.023617525, 0.003...  \n3      [[-0.005602383, -0.0029278726, 0.014461238, 0....  \n4      [[-0.005602383, -0.0029278726, 0.014461238, 0....  \n...                                                  ...  \n24516  [[-0.023273492, -0.0878509, -0.1062212, -0.007...  \n24517  [[-0.023273492, -0.0878509, -0.1062212, -0.007...  \n24518  [[-0.015261493, -0.039154943, 0.05141214, 0.10...  \n24519  [[-0.012049904, -0.064932406, 0.010630969, 0.1...  \n24520  [[-0.034568943, 0.037715632, 0.057920717, 0.02...  \n\n[22364 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UniProtID_1</th>\n      <th>UniProtID_2</th>\n      <th>symbol_1</th>\n      <th>symbol_2</th>\n      <th>seq_1</th>\n      <th>seq_2</th>\n      <th>Experimental System</th>\n      <th>Throughput</th>\n      <th>len_1</th>\n      <th>len_2</th>\n      <th>Pubmed ID</th>\n      <th>wild_seq_1_embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P67809</td>\n      <td>Q13242</td>\n      <td>YBOX1_HUMAN</td>\n      <td>SRSF9_HUMAN</td>\n      <td>MSSEAETQQPPAAPPAAPALSAADTKPGTTGSGAGSGGPGGLTSAA...</td>\n      <td>MSGWADERGGEGDGRIYVGNLPTDVREKDLEDLFYKYGRIREIELK...</td>\n      <td>Two-hybrid</td>\n      <td>Low Throughput</td>\n      <td>324</td>\n      <td>221</td>\n      <td>12604611.0</td>\n      <td>[[0.020276865, -0.055094432, -0.05752476, 0.06...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q8NER1</td>\n      <td>Q86SS6</td>\n      <td>TRPV1_HUMAN</td>\n      <td>SYT9_HUMAN</td>\n      <td>MKKWSSTDLGAAADPLQKDTCPDPLDGDPNSRPPPAKPQLSTAKSR...</td>\n      <td>MPGARDALCHQALQLLAELCARGALEHDSCQDFIYHLRDRARPRLR...</td>\n      <td>Two-hybrid</td>\n      <td>Low Throughput</td>\n      <td>839</td>\n      <td>491</td>\n      <td>15066994.0</td>\n      <td>[[0.005592045, -0.07453048, -0.054872476, 0.06...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O75319</td>\n      <td>Q13242</td>\n      <td>DUS11_HUMAN</td>\n      <td>SRSF9_HUMAN</td>\n      <td>MRNSETLERGVGGCRVFSCLGSYPGIEGAGLALLADLALGGRLLGT...</td>\n      <td>MSGWADERGGEGDGRIYVGNLPTDVREKDLEDLFYKYGRIREIELK...</td>\n      <td>Two-hybrid</td>\n      <td>Low Throughput</td>\n      <td>377</td>\n      <td>221</td>\n      <td>9685386.0</td>\n      <td>[[-0.006202125, -0.0743498, 0.023617525, 0.003...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q13526</td>\n      <td>Q7Z5J4</td>\n      <td>PIN1_HUMAN</td>\n      <td>RAI1_HUMAN</td>\n      <td>MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNSSSGGK...</td>\n      <td>MQSFRERCGFHGKQQNYQQTSQETSRLENYRQPSQAGLSCDRQRLL...</td>\n      <td>Two-hybrid</td>\n      <td>High Throughput</td>\n      <td>163</td>\n      <td>1906</td>\n      <td>16189514.0</td>\n      <td>[[-0.005602383, -0.0029278726, 0.014461238, 0....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q13526</td>\n      <td>Q9UJY4</td>\n      <td>PIN1_HUMAN</td>\n      <td>GGA2_HUMAN</td>\n      <td>MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNSSSGGK...</td>\n      <td>MAATAVAAAVAGTESAQGPPGPAASLELWLNKATDPSMSEQDWSAI...</td>\n      <td>Two-hybrid</td>\n      <td>High Throughput</td>\n      <td>163</td>\n      <td>613</td>\n      <td>16189514.0</td>\n      <td>[[-0.005602383, -0.0029278726, 0.014461238, 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24516</th>\n      <td>P60763</td>\n      <td>Q13443</td>\n      <td>RAC3_HUMAN</td>\n      <td>ADAM9_HUMAN</td>\n      <td>MQAIKCVVVGDGAVGKTCLLISYTTNAFPGEYIPTVFDNYSANVMV...</td>\n      <td>MGSGARFPSGTLRVRWLLLLGLVGPVLGAARPGFQQTSHLSSYEII...</td>\n      <td>Affinity Capture-MS</td>\n      <td>Low Throughput</td>\n      <td>192</td>\n      <td>819</td>\n      <td>31871319.0</td>\n      <td>[[-0.023273492, -0.0878509, -0.1062212, -0.007...</td>\n    </tr>\n    <tr>\n      <th>24517</th>\n      <td>P60763</td>\n      <td>Q9UNK0</td>\n      <td>RAC3_HUMAN</td>\n      <td>STX8_HUMAN</td>\n      <td>MQAIKCVVVGDGAVGKTCLLISYTTNAFPGEYIPTVFDNYSANVMV...</td>\n      <td>MAPDPWFSTYDSTCQIAQEIAEKIQQRNQYERKGEKAPKLTVTIRA...</td>\n      <td>Affinity Capture-MS</td>\n      <td>Low Throughput</td>\n      <td>192</td>\n      <td>236</td>\n      <td>31871319.0</td>\n      <td>[[-0.023273492, -0.0878509, -0.1062212, -0.007...</td>\n    </tr>\n    <tr>\n      <th>24518</th>\n      <td>Q9BQ90</td>\n      <td>Q15843</td>\n      <td>KLDC3_HUMAN</td>\n      <td>NEDD8_HUMAN</td>\n      <td>MLRWTVHLEGGPRRVNHAAVAVGHRVYSFGGYCSGEDYETLRQIDV...</td>\n      <td>MLIKVKTLTGKEIEIDIEPTDKVERIKERVEEKEGIPPQQQRLIYS...</td>\n      <td>Affinity Capture-MS</td>\n      <td>Low Throughput</td>\n      <td>382</td>\n      <td>81</td>\n      <td>35468939.0</td>\n      <td>[[-0.015261493, -0.039154943, 0.05141214, 0.10...</td>\n    </tr>\n    <tr>\n      <th>24519</th>\n      <td>Q9BWK5</td>\n      <td>Q9H9Q4</td>\n      <td>CYREN_HUMAN</td>\n      <td>NHEJ1_HUMAN</td>\n      <td>METLQSETKTRVLPSWLTAQVATKNVAPMKAPKRMRMAAVPVAAAR...</td>\n      <td>MEELEQGLLMQPWAWLQLAENSLLAKVFITKQGYALLVSDLQQVWH...</td>\n      <td>Affinity Capture-Western</td>\n      <td>Low Throughput</td>\n      <td>157</td>\n      <td>299</td>\n      <td>30017584.0</td>\n      <td>[[-0.012049904, -0.064932406, 0.010630969, 0.1...</td>\n    </tr>\n    <tr>\n      <th>24520</th>\n      <td>Q9BR09</td>\n      <td>P13533</td>\n      <td>NEUL2_HUMAN</td>\n      <td>MYH6_HUMAN</td>\n      <td>MAAASEPVDSGALWGLERPEPPPTRFHRVHGANIRVDPSGTRATRV...</td>\n      <td>MTDAQMADFGAAAQYLRKSEKERLEAQTRPFDIRTECFVPDDKEEF...</td>\n      <td>Biochemical Activity</td>\n      <td>Low Throughput</td>\n      <td>285</td>\n      <td>1939</td>\n      <td>35551201.0</td>\n      <td>[[-0.034568943, 0.037715632, 0.057920717, 0.02...</td>\n    </tr>\n  </tbody>\n</table>\n<p>22364 rows  12 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"debug wt sequence 2\"\"\"\n",
    "path = './outputs/variables/'\n",
    "with open(path + 'PPI_seq1_embeddings_full_22364.pk', 'rb') as f:\n",
    "    representation_store_dict = pickle.load(f)\n",
    "sequence_embeddings = {key: np.mean(value, axis=0, keepdims=True) for key, value in representation_store_dict.items()}\n",
    "\n",
    "\n",
    "def update_embeddings(row, embedding_dict):\n",
    "    \"\"\"\n",
    "    add embeddings to the metadata column.\n",
    "    cannot do the reverse, because due to mislabel, several different protein names share the same sequences\n",
    "    but as long as sequences are correct, so will the embeddings\n",
    "    \"\"\"\n",
    "    for key, value in embedding_dict.items():\n",
    "        if row == key:\n",
    "            return value\n",
    "\n",
    "\n",
    "df['wild_seq_1_embeddings'] = df['seq_1'].apply(update_embeddings, embedding_dict=sequence_embeddings)\n",
    "df = df.dropna(subset=['wild_seq_1_embeddings'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n"
     ]
    }
   ],
   "source": [
    "seqs_labeled_wt2 = []\n",
    "seqs_wt2 = df.seq_2.values.tolist()\n",
    "seqs_wt2 = set(seqs_wt2)\n",
    "count = 0\n",
    "for seq in seqs_wt2:\n",
    "    seqs_labeled_wt2.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "# Load ESM-2 model\n",
    "#del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "batch_size = 2000\n",
    "dataset_seq2 = FastaBatchedDataset(list(zip(*seqs_labeled_wt2))[0], list(zip(*seqs_labeled_wt2))[1])\n",
    "batches_seq2 = dataset_seq2.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader_seq2 = torch.utils.data.DataLoader(dataset_seq2, collate_fn=Alphabet.from_architecture(\n",
    "    \"roberta_large\").get_batch_converter(), batch_sampler=batches_seq2, pin_memory=False)\n",
    "len(data_loader_seq2)\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print('Transferred model to GPU')\n",
    "import datetime\n",
    "\n",
    "\n",
    "def tprint(string):\n",
    "    string = str(string)\n",
    "    sys.stdout.write(str(datetime.datetime.now()) + ' | ')\n",
    "    sys.stdout.write(string + '\\n')\n",
    "    sys.stdout.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502 ['seq667'] ['MAKRSRGPGRRCLLALVLFCAWGTLAVVAQKPGAGCPSRCLCFRTTVRCMHLLLEAVPAVAPQTSILDLRFNRIREIQPGAFRRLRNLNTLLLNNNQIKRIPSGAFEDLENLKYLYLYKNEIQSIDRQAFKGLASLEQLYLHFNQIETLDPDSFQHLPKLERLFLHNNRITHLVPGTFNHLESMKRLRLDSNTLHCDCEILWLADLLKTYAESGNAQAAAICEYPRRIQGRSVATITPEELNCERPRITSEPQDADVTSGNTVYFTCRAEGNPKPEIIWLRNNNELSMKTDSRLNLLDDGTLMIQNTQETDQGIYQCMAKNVAGEVKTQEVTLRYFGSPARPTFVIQPQNTEVLVGESVTLECSATGHPPPRISWTRGDRTPLPVDPRVNITPSGGLYIQNVVQGDSGEYACSATNNIDSVHATAFIIVQALPQFTVTPQDRVVIEGQTVDFQCEAKGNPPPVIAWTKGGSQLSVDRRHLVLSSGTLRISGVALHDQGQYECQAVNIIGSQKVVAHLTVQPRVTPVFASIPSDTTVEVGANVQLPCSSQGEPEPAITWNKDGVQVTESGKFHISPEGFLTINDVGPADAGRYECVARNTIGSASVSMVLSVNVPDVSRNGDPFVATSIVEAIATVDRAINSTRTHLFDSRPRSPNDLLALFRYPRDPYTVEQARAGEIFERTLQLIQEHVQHGLMVDLNGTSYHYNDLVSPQYLNLIANLSGCTAHRRVNNCSDMCFHQKYRTHDGTCNNLQHPMWGASLTAFERLLKSVYENGFNTPRGINPHRLYNGHALPMPRLVSTTLIGTETVTPDEQFTHMLMQWGQFLDHDLDSTVVALSQARFSDGQHCSNVCSNDPPCFSVMIPPNDSRARSGARCMFFVRSSPVCGSGMTSLLMNSVYPREQINQLTSYIDASNVYGSTEHEARSIRDLASHRGLLRQGIVQRSGKPLLPFATGPPTECMRDENESPIPCFLAGDHRANEQLGLTSMHTLWFREHNRIATELLKLNPHWDGDTIYYETRKIVGAEIQHITYQHWLPKILGEVGMRTLGEYHGYDPGINAGIFNAFATAAFRFGHTLVNPLLYRLDENFQPIAQDHLPLHKAFFSPFRIVNEGGIDPLLRGLFGVAGKMRVPSQLLNTELTERLFSMAHTVALDLAAINIQRGRDHGIPPYHDYRVYCNLSAAHTFEDLKNEIKNPEIREKLKRLYGSTLNIDLFPALVVEDLVPGSRLGPTLMCLLSTQFKRLRDGDRLWYENPGVFSPAQLTQIKQTSLARILCDNADNITRVQSDVFRVAEFPHGYGSCDEIPRVDLRVWQDCCEDCRTRGQFNAFSYHFRGRRSLEFSYQEDKPTKKTRPRKIPSVGRQGEHLSNSTSAFSTRSDASGTNDFREFVLEMQKTITDLRTQIKKLESRLSTTECVDAGGESHANNTKWKKDACTICECKDGQVTCFVEACPPATCAVPVNIPGACCPVCLQKRAEEKP'] 1479\n",
      "1503 ['seq1961'] ['MGPGRPAPAPWPRHLLRCVLLLGCLHLGRPGAPGDAALPEPNVFLIFSHGLQGCLEAQGGQVRVTPACNTSLPAQRWKWVSRNRLFNLGTMQCLGTGWPGTNTTASLGMYECDREALNLRWHCRTLGDQLSLLLGARTSNISKPGTLERGDQTRSGQWRIYGSEEDLCALPYHEVYTIQGNSHGKPCTIPFKYDNQWFHGCTSTGREDGHLWCATTQDYGKDERWGFCPIKSNDCETFWDKDQLTDSCYQFNFQSTLSWREAWASCEQQGADLLSITEIHEQTYINGLLTGYSSTLWIGLNDLDTSGGWQWSDNSPLKYLNWESDQPDNPSEENCGVIRTESSGGWQNRDCSIALPYVCKKKPNATAEPTPPDRWANVKVECEPSWQPFQGHCYRLQAEKRSWQESKKACLRGGGDLVSIHSMAELEFITKQIKQEVEELWIGLNDLKLQMNFEWSDGSLVSFTHWHPFEPNNFRDSLEDCVTIWGPEGRWNDSPCNQSLPSICKKAGQLSQGAAEEDHGCRKGWTWHSPSCYWLGEDQVTYSEARRLCTDHGSQLVTITNRFEQAFVSSLIYNWEGEYFWTALQDLNSTGSFFWLSGDEVMYTHWNRDQPGYSRGGCVALATGSAMGLWEVKNCTSFRARYICRQSLGTPVTPELPGPDPTPSLTGSCPQGWASDTKLRYCYKVFSSERLQDKKSWVQAQGACQELGAQLLSLASYEEEHFVANMLNKIFGESEPEIHEQHWFWIGLNRRDPRGGQSWRWSDGVGFSYHNFDRSRHDDDDIRGCAVLDLASLQWVAMQCDTQLDWICKIPRGTDVREPDDSPQGRREWLRFQEAEYKFFEHHSTWAQAQRICTWFQAELTSVHSQAELDFLSHNLQKFSRAQEQHWWIGLHTSESDGRFRWTDGSIINFISWAPGKPRPVGKDKKCVYMTASREDWGDQRCLTALPYICKRSNVTKETQPPDLPTTALGGCPSDWIQFLNKCFQVQGQEPQSRVKWSEAQFSCEQQEAQLVTITNPLEQAFITASLPNVTFDLWIGLHASQRDFQWVEQEPLMYANWAPGEPSGPSPAPSGNKPTSCAVVLHSPSAHFTGRWDDRSCTEETHGFICQKGTDPSLSPSPAALPPAPGTELSYLNGTFRLLQKPLRWHDALLLCESRNASLAYVPDPYTQAFLTQAARGLRTPLWIGLAGEEGSRRYSWVSEEPLNYVGWQDGEPQQPGGCTYVDVDGAWRTTSCDTKLQGAVCGVSSGPPPPRRISYHGSCPQGLADSAWIPFREHCYSFHMELLLGHKEARQRCQRAGGAVLSILDEMENVFVWEHLQSYEGQSRGAWLGMNFNPKGGTLVWQDNTAVNYSNWGPPGLGPSMLSHNSCYWIQSNSGLWRPGACTNITMGVVCKLPRAEQSSFSPSALPENPAALVVVLMAVLLLLALLTAALILYRRRQSIERGAFEGARYSRSSSSPTEATEKNILVSDMEMNEQQE'] 1479\n",
      "1504 ['seq4475'] ['MSKTLKKKKHWLSKVQECAVSWAGPPGDFGAEIRGGAERGEFPYLGRLREEPGGGTCCVVSGKAPSPGDVLLEVNGTPVSGLTNRDTLAVIRHFREPIRLKTVKPGKVINKDLRHYLSLQFQKGSIDHKLQQVIRDNLYLRTIPCTTRAPRDGEVPGVDYNFISVEQFKALEESGALLESGTYDGNFYGTPKPPAEPSPFQPDPVDQVLFDNEFDAESQRKRTTSVSKMERMDSSLPEEEEDEDKEAINGSGNAENRERHSESSDWMKTVPSYNQTNSSMDFRNYMMRDETLEPLPKNWEMAYTDTGMIYFIDHNTKTTTWLDPRLCKKAKAPEDCEDGELPYGWEKIEDPQYGTYYVDHLNQKTQFENPVEEAKRKKQLGQVEIGSSKPDMEKSHFTRDPSQLKGVLVRASLKKSTMGFGFTIIGGDRPDEFLQVKNVLKDGPAAQDGKIAPGDVIVDINGNCVLGHTHADVVQMFQLVPVNQYVNLTLCRGYPLPDDSEDPVVDIVAATPVINGQSLTKGETCMNPQDFKPGAMVLEQNGKSGHTLTGDGLNGPSDASEQRVSMASSGSSQPELVTIPLIKGPKGFGFAIADSPTGQKVKMILDSQWCQGLQKGDIIKEIYHQNVQNLTHLQVVEVLKQFPVGADVPLLILRGGPPSPTKTAKMKTDKKENAGSLEAINEPIPQPMPFPPSIIRSGSPKLDPSEVYLKSKTLYEDKPPNTKDLDVFLRKQESGFGFRVLGGDGPDQSIYIGAIIPLGAAEKDGRLRAADELMCIDGIPVKGKSHKQVLDLMTTAARNGHVLLTVRRKIFYGEKQPEDDSSQAFISTQNGSPRLNRAEVPARPAPQEPYDVVLQRKENEGFGFVILTSKNKPPPGVIPHKIGRVIEGSPADRCGKLKVGDHISAVNGQSIVELSHDNIVQLIKDAGVTVTLTVIAEEEHHGPPSGTNSARQSPALQHRPMGQSQANHIPGDRSALEGEIGKDVSTSYRHSWSDHKHLAQPDTAVISVVGSRHNQNLGCYPVELERGPRGFGFSLRGGKEYNMGLFILRLAEDGPAIKDGRIHVGDQIVEINGEPTQGITHTRAIELIQAGGNKVLLLLRPGTGLIPDHGDWDINNPSSSNVIYDEQSPLPPSSHFASIFEESHVPVIEESLRVQICEKAEELKDIVPEKKSTLNENQPEIKHQSLLQKNVSKRDPPSSHGHSNKKNLLKVENGVTRRGRSVSPKKPASQHSEEHLDKIPSPLKNNPKRRPRDQSLSPSKGENKSCQVSTRAGSGQDQCRKSRGRSASPKKQQKIEGSKAPSNAEAKLLEGKSRRIAGYTGSNAEQIPDGKEKSDVIRKDAKQNQLEKSRTRSPEKKIKRMVEKSLPSKMTNKTTSKEVSENEKGKKVTTGETSSSNDKIGENVQLSEKRLKQEPEEKVVSNKTEDHKGKELEAADKNKETGRFKPESSSPVKKTLITPGPWKVPSGNKVTGTIGMAEKRQ'] 1481\n",
      "1505 ['seq2951'] ['MAPLLGRKPFPLVKPLPGEEPLFTIPHTQEAFRTREEYEARLERYSERIWTCKSTGSSQLTHKEAWEEEQEVAELLKEEFPAWYEKLVLEMVHHNTASLEKLVDTAWLEIMTKYAVGEECDFEVGKEKMLKVKIVKIHPLEKVDEEATEKKSDGACDSPSSDKENSSQIAQDHQKKETVVKEDEGRRESINDRARRSPRKLPTSLKKGERKWAPPKFLPHKYDVKLQNEDKIISNVPADSLIRTERPPNKEIVRYFIRHNALRAGTGENAPWVVEDELVKKYSLPSKFSDFLLDPYKYMTLNPSTKRKNTGSPDRKPSKKSKTDNSSLSSPLNPKLWCHVHLKKSLSGSPLKVKNSKNSKSPEEHLEEMMKMMSPNKLHTNFHIPKKGPPAKKPGKHSDKPLKAKGRSKGILNGQKSTGNSKSPKKGLKTPKTKMKQMTLLDMAKGTQKMTRAPRNSGGTPRTSSKPHKHLPPAALHLIAYYKENKDREDKRSALSCVISKTARLLSSEDRARLPEELRSLVQKRYELLEHKKRWASMSEEQRKEYLKKKREELKKKLKEKAKERREKEMLERLEKQKRYEDQELTGKNLPAFRLVDTPEGLPNTLFGDVAMVVEFLSCYSGLLLPDAQYPITAVSLMEALSADKGGFLYLNRVLVILLQTLLQDEIAEDYGELGMKLSEIPLTLHSVSELVRLCLRRSDVQEESEGSDTDDNKDSAAFEDNEVQDEFLEKLETSEFFELTSEEKLQILTALCHRILMTYSVQDHMETRQQMSAELWKERLAVLKEENDKKRAEKQKRKEMEAKNKENGKVENGLGKTDRKKEIVKFEPQVDTEAEDMISAVKSRRLLAIQAKKEREIQEREMKVKLERQAEEERIRKHKAAAEKAFQEGIAKAKLVMRRTPIGTDRNHNRYWLFSDEVPGLFIEKGWVHDSIDYRFNHHCKDHTVSGDEDYCPRSKKANLGKNASMNTQHGTATEVAVETTTPKQGQNLWFLCDSQKELDELLNCLHPQGIRESQLKERLEKRYQDIIHSIHLARKPNLGLKSCDGNQELLNFLRSDLIEVATRLQKGGLGYVEETSEFEARVISLEKLKDFGECVIALQASVIKKFLQGFMAPKQKRRKLQSEDSAKTEEVDEEKKMVEEAKVASALEKWKTAIREAQTFSRMHVLLGMLDACIKWDMSAENARCKVCRKKGEDDKLILCDECNKAFHLFCLRPALYEVPDGEWQCPACQPATARRNSRGRNYTEESASEDSEDDESDEEEEEEEEEEEEEDYEVAGLRLRPRKTIRGKHSVIPPAARSGRRPGKKPHSTRRSQPKAPPVDDAEVDELVLQTKRSSRRQSLELQKCEEILHKIVKYRFSWPFREPVTRDEAEDYYDVITHPMDFQTVQNKCSCGSYRSVQEFLTDMKQVFTNAEVYNCRGSHVLSCMVKTEQCLVALLHKHLPGHPYVRRKRKKFPDRLAEDEGDSEPEAVGQSRGRRQKK'] 1483\n"
     ]
    }
   ],
   "source": [
    "# QC\n",
    "start_batch_id = 1502\n",
    "end_batch_id = 1505\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader_seq2):\n",
    "    if start_batch_id <= batch_idx <= end_batch_id:\n",
    "    #if len(strs[0]) == 1538:\n",
    "        print(batch_idx, labels, strs, len(strs[0]))\n",
    "        #break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 11:29:31.572825 | Batch 1502: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:31.707259 | Batch 1502: Allocated memory increased by 0.03 GB, Cached memory increased by 32.61 GB\n",
      "2023-09-05 11:29:31.915605 | Batch 1502: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:31.978152 | Batch 1502: Allocated memory increased by -0.01 GB, Cached memory increased by -32.58 GB\n",
      "2023-09-05 11:29:31.978735 | Batch ID: 1502['seq1871']1\n",
      "2023-09-05 11:29:31.979614 | Batch 1502: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:31.979892 | finish generating embeddings\n",
      "2023-09-05 11:29:31.988615 | Batch 1503: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:32.842327 | Batch 1503: Allocated memory increased by 0.01 GB, Cached memory increased by 32.58 GB\n",
      "2023-09-05 11:29:32.922521 | Batch 1503: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:33.193509 | Batch 1503: Allocated memory increased by -0.01 GB, Cached memory increased by -32.58 GB\n",
      "2023-09-05 11:29:33.194171 | Batch ID: 1503['seq4433']1\n",
      "2023-09-05 11:29:33.194983 | Batch 1503: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:33.195304 | finish generating embeddings\n",
      "2023-09-05 11:29:33.204170 | Batch 1504: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:34.054923 | Batch 1504: Allocated memory increased by 0.01 GB, Cached memory increased by 32.66 GB\n",
      "2023-09-05 11:29:34.160147 | Batch 1504: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:34.921221 | Batch 1504: Allocated memory increased by -0.01 GB, Cached memory increased by -32.66 GB\n",
      "2023-09-05 11:29:34.921812 | Batch ID: 1504['seq432']1\n",
      "2023-09-05 11:29:34.922603 | Batch 1504: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:34.922907 | finish generating embeddings\n",
      "2023-09-05 11:29:34.931525 | Batch 1505: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:35.302848 | Batch 1505: Allocated memory increased by 0.01 GB, Cached memory increased by 32.79 GB\n",
      "2023-09-05 11:29:35.410251 | Batch 1505: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:35.495291 | Batch 1505: Allocated memory increased by -0.01 GB, Cached memory increased by -32.79 GB\n",
      "2023-09-05 11:29:35.495792 | Batch ID: 1505['seq845']1\n",
      "2023-09-05 11:29:35.497126 | Batch 1505: Allocated memory increased by 0.00 GB, Cached memory increased by 0.00 GB\n",
      "2023-09-05 11:29:35.497647 | finish generating embeddings\n"
     ]
    }
   ],
   "source": [
    "representation_store_dict = {}\n",
    "start_batch_id = 1502\n",
    "end_batch_id = 1505\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader_seq2):\n",
    "    start_allocated, start_cached = get_gpu_memory()\n",
    "    if start_batch_id <= batch_idx <= end_batch_id:\n",
    "        if torch.cuda.is_available():\n",
    "            toks = toks.to(device='cuda', non_blocking=True)\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(\n",
    "            f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        with torch.no_grad():\n",
    "            results = model(toks, repr_layers=[33], return_contacts=True)['representations'][33]\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(\n",
    "            f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        results_cpu = results.to(device='cpu')\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(\n",
    "            f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        del results, toks\n",
    "        torch.cuda.empty_cache()\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(\n",
    "            f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        tprint('Batch ID: ' + str(batch_idx) + str(labels) + str(len(strs)))\n",
    "        #tprint(torch.cuda.memory_allocated())\n",
    "        #tprint(torch.cuda.memory_snapshot())\n",
    "        start_allocated, start_cached = get_gpu_memory()\n",
    "        for i, str_ in enumerate(strs):\n",
    "            # only select representations relate to the sequence\n",
    "            # rest of the sequences are paddings, check notebook\n",
    "            # create dictionary {sequence: embeddings}\n",
    "            representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i]) + 1)].numpy()\n",
    "        end_allocated, end_cached = get_gpu_memory()\n",
    "        tprint(\n",
    "            f\"Batch {batch_idx}: Allocated memory increased by {(end_allocated - start_allocated):.2f} GB, Cached memory increased by {(end_cached - start_cached):.2f} GB\")\n",
    "        tprint('finish generating embeddings')\n",
    "    elif batch_idx > end_batch_id:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}