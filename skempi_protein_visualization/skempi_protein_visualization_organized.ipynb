{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "# pip install tables\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retrieve all proteins from the dataset\n",
    "#df = pd.read_csv('USE__mutant_seqswoscore.csv', index_col=0)\n",
    "df = pd.read_csv('skempiwmutants_nanincl.csv', index_col=0)\n",
    "df = df[['#Pdb','Mutation(s)_PDB', 'Affinity_mut_parsed','Affinity_wt_parsed','Protein 1', 'Protein 2', 'wild_seq1', 'wild_seq2', 'mutant_seq']]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if there's \":\" in the column, just drop first, indicating multi-chain situation\n",
    "ignore = (df['wild_seq1'].str.contains(':')) | (df['wild_seq2'].str.contains(':')) | (df['mutant_seq'].str.contains(':'))\n",
    "reduced_df = df[~ignore]\n",
    "reduced_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check and remove if there's any duplicates\n",
    "reduced_df.drop_duplicates(inplace=True)\n",
    "df_ = reduced_df.copy()\n",
    "# shouldn't drop na, na means lose binding ability in mutants\n",
    "#df_.dropna(inplace=True)\n",
    "df_['wild_seq_1'] = df_['wild_seq1']\n",
    "df_['wild_seq_2'] = df_['wild_seq2']\n",
    "df_['Protein_1'] = df_['Protein 1']\n",
    "df_['Protein_2'] = df_['Protein 2']\n",
    "df_ = df_.reset_index(drop=True)\n",
    "df_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This chunk should re-organize the dataframe so that:\n",
    "    1. all the sequence listed in wt_seq2 will have positions mutated\n",
    "    2. if the wt_seq2 and wt_seq1 flipped, so well columns [Protein1, Protein2]\n",
    "\"\"\"\n",
    "for index, row in df_.iterrows():\n",
    "    if fuzz.ratio(row['wild_seq1'], row['mutant_seq']) > fuzz.ratio(row['wild_seq2'], row['mutant_seq']) :\n",
    "        # then the two sequences are similar\n",
    "        # flip the sequence 1 to sequence 2\n",
    "            df_.at[index, 'wild_seq_1'] = row['wild_seq2']\n",
    "            df_.at[index, 'wild_seq_2'] = row['wild_seq1']\n",
    "            df_.at[index, 'Protein_1'] = row['Protein 2']\n",
    "            df_.at[index, 'Protein_2'] = row['Protein 1']\n",
    "    elif fuzz.ratio(row['wild_seq1'], row['mutant_seq'])  < fuzz.ratio(row['wild_seq2'], row['mutant_seq']) :\n",
    "        pass\n",
    "    else:\n",
    "        print(index, fuzz.ratio(row['wild_seq1'], row['mutant_seq']), fuzz.ratio(row['wild_seq2'], row['mutant_seq']))\n",
    "        print('mutate both sequences?')\n",
    "df_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_.drop(columns=['Protein 1', 'Protein 2', 'wild_seq1', 'wild_seq2'], inplace=True)\n",
    "cols = ['#Pdb', 'Mutation(s)_PDB', 'Affinity_mut_parsed', 'Affinity_wt_parsed','Protein_1',\n",
    "        'Protein_2','wild_seq_1','wild_seq_2', 'mutant_seq']\n",
    "df_ = df_[cols]\n",
    "df_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate sequence embeddings to the proteins\n",
    "seqs_wt1 = df_.wild_seq_1.values.tolist()\n",
    "seqs_wt2 = df_.wild_seq_2.values.tolist()\n",
    "seqs_mut = df_.mutant_seq.values.tolist()\n",
    "seqs_wt1 = set(seqs_wt1)\n",
    "seqs_wt2 = set(seqs_wt2)\n",
    "seqs_mut = set(seqs_mut)\n",
    "seqs_mut"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"lazy to write function, may need to modify in the future\"\"\"\n",
    "seqs_labeled_wt1 = []\n",
    "count = 0\n",
    "for seq in seqs_wt1:\n",
    "    seqs_labeled_wt1.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "seqs_labeled_wt2 = []\n",
    "count = 0\n",
    "for seq in seqs_wt2:\n",
    "    seqs_labeled_wt2.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1\n",
    "seqs_labeled_mut = []\n",
    "count = 0\n",
    "for seq in seqs_mut:\n",
    "    seqs_labeled_mut.append(tuple((str('seq' + str(count)), seq)))\n",
    "    count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alternative way to generate batches\n",
    "from torch.utils.data import TensorDataset\n",
    "from esm import Alphabet, FastaBatchedDataset\n",
    "batch_size = 1000\n",
    "dataset = FastaBatchedDataset(list(zip(*seqs_labeled_wt1))[0], list(zip(*seqs_labeled_wt1))[1])\n",
    "batches = dataset.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, collate_fn=Alphabet.from_architecture(\"roberta_large\").get_batch_converter(),\n",
    "            batch_sampler=batches, pin_memory=True)\n",
    "dataset_seq2 = FastaBatchedDataset(list(zip(*seqs_labeled_wt2))[0], list(zip(*seqs_labeled_wt2))[1])\n",
    "batches_seq2 = dataset_seq2.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader_seq2 = torch.utils.data.DataLoader(dataset_seq2, collate_fn=Alphabet.from_architecture(\"roberta_large\").get_batch_converter(),\n",
    "            batch_sampler=batches_seq2, pin_memory=True)\n",
    "dataset_mut = FastaBatchedDataset(list(zip(*seqs_labeled_mut))[0], list(zip(*seqs_labeled_mut))[1])\n",
    "batches_mut = dataset_mut.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
    "data_loader_mut = torch.utils.data.DataLoader(dataset_mut, collate_fn=Alphabet.from_architecture(\"roberta_large\").get_batch_converter(),\n",
    "            batch_sampler=batches_mut, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print('Transferred model to GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#QC\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    print(batch_idx,labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "representation_store_dict = {}\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        toks = toks.to(device='cuda', non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "        #results = model(toks, repr_layers = [33], return_contacts = True)['logits']\n",
    "    #print(results.shape)\n",
    "    results_cpu = results.to(device='cpu')\n",
    "    for i, str_ in enumerate(strs):\n",
    "        representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take the average of the representations of the proteins\n",
    "# for umaps, along axis 0\n",
    "sequence_embeddings = {key: np.mean(value, axis=0, keepdims=True) for key, value in representation_store_dict.items()}\n",
    "#print({key: value.shape for key, value in sequence_embeddings.items()})\n",
    "sequence_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "representation_store_dict_seq2 = {}\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader_seq2):\n",
    "    if torch.cuda.is_available():\n",
    "        toks = toks.to(device='cuda', non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "        #results = model(toks, repr_layers = [33], return_contacts = True)['logits']\n",
    "    results_cpu = results.to(device='cpu')\n",
    "    for i, str_ in enumerate(strs):\n",
    "        representation_store_dict_seq2[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()\n",
    "sequence_embeddings_seq2 = {key: np.mean(value, axis=0, keepdims=True) for key, value in representation_store_dict_seq2.items()}\n",
    "representation_store_dict_mut = {}\n",
    "for batch_idx, (labels, strs, toks) in enumerate(data_loader_mut):\n",
    "    if torch.cuda.is_available():\n",
    "        toks = toks.to(device='cuda', non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
    "        #results = model(toks, repr_layers = [33], return_contacts = True)['logits']\n",
    "    results_cpu = results.to(device='cpu')\n",
    "    for i, str_ in enumerate(strs):\n",
    "        representation_store_dict_mut[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()\n",
    "sequence_embeddings_mut = {key: np.mean(value, axis=0, keepdims=True) for key, value in representation_store_dict_mut.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_embeddings(row, embedding_dict):\n",
    "    \"\"\"\n",
    "    add embeddings to the metadata column.\n",
    "    cannot do the reverse, because due to mislabel, several different protein names share the same sequences\n",
    "    but as long as sequences are correct, so will the embeddings\n",
    "    \"\"\"\n",
    "    for key, value in embedding_dict.items():\n",
    "        if row == key:\n",
    "            return value\n",
    "df_['wild_seq_1_embeddings'] = df_['wild_seq_1'].apply(update_embeddings, embedding_dict=sequence_embeddings)\n",
    "df_['wild_seq_2_embeddings'] = df_['wild_seq_2'].apply(update_embeddings, embedding_dict=sequence_embeddings_seq2)\n",
    "df_['mutant_seq_embeddings'] = df_['mutant_seq'].apply(update_embeddings, embedding_dict=sequence_embeddings_mut)\n",
    "df_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_.to_hdf('./outputs/dataframes/proteins_embeddings_meta.hdf', key='df', mode='w')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extract meta information to draw umaps\n",
    "# need to reorganize dataframe to make the features clear for each protein sequence\n",
    "ppi = df_['Protein_1'] + '---' + df_['Protein_2']\n",
    "prot_1 = df_['Protein_1']\n",
    "prot_2 = df_['Protein_2']\n",
    "prot_mut = df_['Protein_2']+'_'+'mut'\n",
    "seq_1 = df_['wild_seq_1']\n",
    "seq_2 = df_['wild_seq_2']\n",
    "seq_mut = df_['mutant_seq']\n",
    "embed_1 = df_['wild_seq_1_embeddings']\n",
    "embed_2 = df_['wild_seq_2_embeddings']\n",
    "embed_mut = df_['mutant_seq_embeddings']\n",
    "affinity_wt = df_['Affinity_wt_parsed']\n",
    "affinity_mut = df_['Affinity_mut_parsed']\n",
    "label_wt_1 = pd.Series(['wt1']*len(prot_1))\n",
    "label_wt_2 = pd.Series(['wt2']*len(prot_2))\n",
    "label_mut = pd.Series(['mut']*len(prot_mut))\n",
    "pdbs = df_['#Pdb']\n",
    "mut = df_['Mutation(s)_PDB']\n",
    "mut_status_wt = pd.Series([np.nan]*len(prot_1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame({\n",
    "    'PDB': pd.concat([pdbs, pdbs, pdbs], ignore_index=True),\n",
    "    'Protein': pd.concat([prot_1, prot_2, prot_mut], ignore_index=True),\n",
    "    'Mutation': pd.concat([mut_status_wt, mut_status_wt, mut], ignore_index=True),\n",
    "    'Sequence': pd.concat([seq_1, seq_2, seq_mut], ignore_index=True),\n",
    "    'PPI': pd.concat([ppi, ppi, ppi], ignore_index=True),\n",
    "    'Label': pd.concat([label_wt_1, label_wt_2, label_mut], ignore_index=True),\n",
    "    'Affinity': pd.concat([affinity_wt, affinity_mut, affinity_mut], ignore_index=True),\n",
    "    'Embedding': pd.concat([embed_1, embed_2, embed_mut], ignore_index=True),\n",
    "})\n",
    "df_meta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expanded_embeddings = df_meta['Embedding'].apply(lambda x: pd.Series(x[0]))\n",
    "df_umap = pd.concat([df_meta, expanded_embeddings], axis=1)\n",
    "df_umap.drop(['Embedding'], axis=1, inplace=True)\n",
    "df_umap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_umap.drop_duplicates(inplace=True, ignore_index=True)\n",
    "# weird duplicates should be dropped under the affinity column\n",
    "# keep the first encounter first\n",
    "# seems like a lot of wts mistakenly have affinities of mutants\n",
    "#df_umap= df_umap.drop_duplicates(subset='Affinity', keep='first', ignore_index=True)\n",
    "df_umap.drop_duplicates(subset=['Sequence','Affinity'], keep='first')\n",
    "df_umap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = df_umap.loc[:,0:]\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "proj_2d = umap_2d.fit_transform(features)\n",
    "results = {'umap1': proj_2d[:, 0], 'umap2': proj_2d[:, 1]}\n",
    "umap_results = pd.DataFrame(data=results)\n",
    "umap_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_umap_ = pd.concat([df_umap, umap_results], axis=1)\n",
    "df_umap_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color_dict = {'wt1':'#72B7B2', 'wt2':'#54A24B', 'mut':'#E45756'}\n",
    "fig= px.scatter(\n",
    "    df_umap_,\n",
    "    x = 'umap1',\n",
    "    y = 'umap2',\n",
    "    color='Label',\n",
    "    color_discrete_map=color_dict,\n",
    "    hover_name='Protein',\n",
    "    hover_data={\n",
    "        'Protein': False,\n",
    "        'PDB': True,\n",
    "        'Label': True,\n",
    "        'PPI': True,\n",
    "        'Mutation':True,\n",
    "        'Affinity':True,\n",
    "        'umap1':False,\n",
    "        'umap2':False\n",
    "    }\n",
    ")\n",
    "fig.update_layout(template='simple_white',\n",
    "                  title='SKEMPI Protein-Protein embeddings',\n",
    "                  title_x=0.5\n",
    "                  )\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig.write_html(\"./outputs/figures/skempi_ppi_plotly.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}