{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ynuozhang/mutant_specific/blob/master/PickleEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4HXK8jS-vN"
      },
      "source": [
        "# Old Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ddhkjIQtkOw",
        "outputId": "badbffab-5dbb-41d6-c15c-a7ff15c08730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\n",
            "Collecting linear-operator>=0.5.0 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (1.11.2)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading jaxtyping-0.2.21-py3-none-any.whl (25 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.23.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.9->linear-operator>=0.5.0->gpytorch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.5.0->gpytorch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.3.0)\n",
            "Installing collected packages: typeguard, jaxtyping, linear-operator, gpytorch\n",
            "Successfully installed gpytorch-1.11 jaxtyping-0.2.21 linear-operator-0.5.2 typeguard-2.13.3\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.1)\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (2021.10.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86770 sha256=83618adb1840d7db1640289afd75bbdf36a02ee856b563a65e0591c5483a23dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=e2d9dbfdadef9c62e708da338f2a57295a72b114d96fc546b930a069fa4f8cd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.4\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ],
      "source": [
        "#@title Installs\n",
        "\n",
        "!pip install gpytorch\n",
        "!pip install fair-esm\n",
        "!pip install torch  # This is for PyTorch\n",
        "!pip install umap-learn  # This installs the UMAP library\n",
        "!pip install biopython\n",
        "\n",
        "\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import gpytorch\n",
        "import torch\n",
        "import umap\n",
        "from esm import Alphabet, FastaBatchedDataset\n",
        "from Bio import SeqIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6KF-p6MpHy9",
        "outputId": "1f8258d1-92e6-40f0-ed16-32a4533b97ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#@title Extract .tar.gz files to /Mutant_Specificity/extracted_fake_seqs\\n\\nfrom google.colab import drive\\nimport tarfile\\nimport os\\n\\n# Mount Google Drive\\ndrive.mount(\\'/content/drive\\')\\n\\n# Define the file path\\nfile_path = \\'/content/drive/MyDrive/Ashley/Mutant_Specificity/natural_fastas.tar.gz\\'\\ndestination_path = \"/content/drive/MyDrive/Ashley/Mutant_Specificity/extracted_fake_seqs\"\\n\\n# Create the directory if it doesn\\'t exist\\nif not os.path.exists(destination_path):\\n    os.makedirs(destination_path)\\n\\n# Open the tar.gz file and extract\\nextracted_files = []\\nwith tarfile.open(file_path, \\'r:gz\\') as file:\\n    # Extract all contents to the destination_path\\n    for member in file.getmembers():\\n        file.extract(member, path=destination_path)\\n        extracted_files.append(member.name)\\n\\nprint(\"Extraction complete!\")\\nprint(\"\\nList of extracted files:\")\\nfor fname in extracted_files:\\n    print(fname)\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#@title Extract .tar.gz files to /Mutant_Specificity/extracted_fake_seqs\n",
        "\n",
        "from google.colab import drive\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/Ashley/Mutant_Specificity/natural_fastas.tar.gz'\n",
        "destination_path = \"/content/drive/MyDrive/Ashley/Mutant_Specificity/extracted_fake_seqs\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(destination_path):\n",
        "    os.makedirs(destination_path)\n",
        "\n",
        "# Open the tar.gz file and extract\n",
        "extracted_files = []\n",
        "with tarfile.open(file_path, 'r:gz') as file:\n",
        "    # Extract all contents to the destination_path\n",
        "    for member in file.getmembers():\n",
        "        file.extract(member, path=destination_path)\n",
        "        extracted_files.append(member.name)\n",
        "\n",
        "print(\"Extraction complete!\")\n",
        "print(\"\\nList of extracted files:\")\n",
        "for fname in extracted_files:\n",
        "    print(fname)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9REFCEFr13u",
        "outputId": "be710eb5-1201-4ea2-9a8d-2e2aa14286d4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#@title Testing if fasta extraction was successful - print the contents of one extracted file\\n\\n# Assuming the extracted_files list contains the names of the files\\nfasta_files = [f for f in extracted_files if f.endswith(\\'.fasta\\')]\\n\\n# Check if there are any FASTA files in the list\\nif fasta_files:\\n    # Path to the first FASTA file\\n    first_fasta_path = os.path.join(destination_path, fasta_files[0])\\n\\n    with open(first_fasta_path, \\'r\\') as f:\\n        content = f.read()\\n        print(f\"Contents of {fasta_files[0]}:\\n\")\\n        print(content)\\nelse:\\n    print(\"No FASTA files found in the extracted files.\")'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#@title Testing if fasta extraction was successful - print the contents of one extracted file\n",
        "\n",
        "# Assuming the extracted_files list contains the names of the files\n",
        "fasta_files = [f for f in extracted_files if f.endswith('.fasta')]\n",
        "\n",
        "# Check if there are any FASTA files in the list\n",
        "if fasta_files:\n",
        "    # Path to the first FASTA file\n",
        "    first_fasta_path = os.path.join(destination_path, fasta_files[0])\n",
        "\n",
        "    with open(first_fasta_path, 'r') as f:\n",
        "        content = f.read()\n",
        "        print(f\"Contents of {fasta_files[0]}:\\n\")\n",
        "        print(content)\n",
        "else:\n",
        "    print(\"No FASTA files found in the extracted files.\")'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_Et4QkTLv2"
      },
      "source": [
        "# TO TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hw2SKLGnUngF"
      },
      "outputs": [],
      "source": [
        "#@title get fasta sequences\n",
        "\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "\n",
        "# Directory containing the FASTA files\n",
        "directory = '/content/drive/MyDrive/Ashley/Mutant_Specificity/extracted_fake_seqs/natural_fastas'\n",
        "\n",
        "# List to store all sequences\n",
        "all_sequences = []\n",
        "\n",
        "# Loop through every file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.fasta'):  # Check if the file is a FASTA file\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        # Parse the FASTA file and add the sequences to the all_sequences list\n",
        "        sequences = [rec.seq for rec in SeqIO.parse(filepath, 'fasta')]\n",
        "        all_sequences.extend(sequences)\n",
        "\n",
        "# Convert the list of sequences to a set to get unique sequences\n",
        "unique_sequences = set(all_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7prYKVci2PPV",
        "outputId": "199731d5-1a67-44f5-fc3b-8d60c8b2825d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-32): 33 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Setting up ESM\n",
        "import esm\n",
        "\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X-eGlEPItlfe"
      },
      "outputs": [],
      "source": [
        "#@title DataLoader\n",
        "seqs = unique_sequences\n",
        "seqs_labeled = []\n",
        "count=0\n",
        "\n",
        "for seq in seqs:\n",
        "  seqs_labeled.append(tuple((str('seq' + str(count)), seq)))\n",
        "  count+=1\n",
        "\n",
        "dataset = FastaBatchedDataset(list(zip(*seqs_labeled))[0], list(zip(*seqs_labeled))[1])\n",
        "batch_size = 2000\n",
        "batches = dataset.get_batch_indices(batch_size, extra_toks_per_seq=1)\n",
        "data_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                          collate_fn = alphabet.get_batch_converter(),\n",
        "                                          batch_sampler = batches, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "lmn1JXKrQZFU",
        "outputId": "b2043655-1bfa-4273-cbce-467bf49c2f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transferred model to GPU\n"
          ]
        }
      ],
      "source": [
        "#@title Push model to gpu\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "  model=model.cuda()\n",
        "  print('Transferred model to GPU')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9P9hhz72R2E2"
      },
      "outputs": [],
      "source": [
        "#@title representation_store_dict\n",
        "\n",
        "representation_store_dict = {}\n",
        "for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
        "    if torch.cuda.is_available():\n",
        "        toks = toks.to(device='cuda', non_blocking=True)\n",
        "    with torch.no_grad():\n",
        "        results = model(toks, repr_layers = [33], return_contacts = True)['representations'][33]\n",
        "        #results = model(toks, repr_layers = [33], return_contacts = True)['logits']\n",
        "    #print(results.shape)\n",
        "    results_cpu = results.to(device='cpu')\n",
        "    for i, str_ in enumerate(strs):\n",
        "      #only select representations that relate to the sequnce\n",
        "      #rest of the sequences are paddings, check notebook\n",
        "      #create dictionary {sequence: embeddings}\n",
        "      representation_store_dict[str_] = results_cpu[i, 1: (len(strs[i])+1)].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "7-54YZhcR6aO"
      },
      "outputs": [],
      "source": [
        "#@title Get average of the representations of the proteins\n",
        "\n",
        "#for umaps, along axis 0\n",
        "sequence_embeddings = {key:np.mean(value, axis=0, keepdims=True) for key, value in representation_store_dict.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1g1Rlj2SKNQ"
      },
      "outputs": [],
      "source": [
        "#@title Make it into a pickle file\n",
        "\n",
        "import pickle\n",
        "import os, sys\n",
        "path = '/content/drive/MyDrive/Ashley/Mutant_Specificity/'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "with open(path+'full_fake_embeddings.pk', 'wb') as handle:\n",
        "    pickle.dump(representation_store_dict, handle)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1l7hEaXFgf-ab8FYqZjmooYhZuGSduLab",
      "authorship_tag": "ABX9TyMlzwTTMaPoLf7OBOvqZeGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}